{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596809094455",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The more features you have...\n",
    "\n",
    "1. the more likely you are to overfit to the training and validation sets.\n",
    "2. the longer it will take to train your model and optimize hyperparameters.\n",
    "3. slower inference (reasoning predictions relative to features afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "import os\n",
    "\n",
    "clicks = pd.read_parquet('/Users/fred/Google Drive/University/Machine Learning/Kaggle/Datasets/228068_567626_bundle_archive/baseline_data.pqt')\n",
    "\n",
    "data_files = ['count_encodings.pqt',\n",
    "              'catboost_encodings.pqt',\n",
    "              'interactions.pqt',\n",
    "              'past_6hr_events.pqt',\n",
    "              'downloads.pqt',\n",
    "              'time_deltas.pqt',\n",
    "              'svd_encodings.pqt']\n",
    "data_root = '/Users/fred/Google Drive/University/Machine Learning/Kaggle/Datasets/228068_567626_bundle_archive'\n",
    "for file in data_files:\n",
    "    features = pd.read_parquet(os.path.join(data_root, file))\n",
    "    clicks = clicks.join(features)\n",
    "\n",
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "\n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def train_model(train, valid, test=None, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    num_round = 1000\n",
    "    print(\"Training model!\")\n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    if test is not None: \n",
    "        test_pred = bst.predict(test[feature_cols])\n",
    "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "        return bst, valid_score, test_score\n",
    "    else:\n",
    "        return bst, valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training model!\nValidation AUC score: 0.9658334271834417\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\nNow we have 91 features we're using for predictions. With all these features, there is a good chance the model is overfitting the data. We might be able to reduce the overfitting by removing some features. Of course, the model's performance might decrease. But at least we'd be making the model smaller and faster without losing much performance.\\n\""
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Let's look at the baseline score for all the features we've made so far.\n",
    "\n",
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_, baseline_score = train_model(train, valid)\n",
    "\n",
    "\"\"\"\n",
    "Now we have 91 features we're using for predictions. With all these features, there is a good chance the model is overfitting the data. We might be able to reduce the overfitting by removing some features. Of course, the model's performance might decrease. But at least we'd be making the model smaller and faster without losing much performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       ip  app  device  os  channel          click_time      attributed_time  \\\n0   27226    3       1  13      120 2017-11-06 15:13:23                 None   \n1  110007   35       1  13       10 2017-11-06 15:41:07  2017-11-07 08:17:19   \n2    1047    6       1  13      157 2017-11-06 15:42:32                 None   \n3   76270    3       1  13      120 2017-11-06 15:56:17                 None   \n4   57862    3       1  13      120 2017-11-06 15:57:01                 None   \n\n   is_attributed  day  hour  ...  device_channel_svd_0  device_channel_svd_1  \\\n0              0    6    15  ...              0.998937             -0.026614   \n1              1    6    15  ...              0.998937             -0.026614   \n2              0    6    15  ...              0.998937             -0.026614   \n3              0    6    15  ...              0.998937             -0.026614   \n4              0    6    15  ...              0.998937             -0.026614   \n\n   device_channel_svd_2  device_channel_svd_3  device_channel_svd_4  \\\n0              0.033651             -0.016794              0.001659   \n1              0.033651             -0.016794              0.001659   \n2              0.033651             -0.016794              0.001659   \n3              0.033651             -0.016794              0.001659   \n4              0.033651             -0.016794              0.001659   \n\n   os_channel_svd_0  os_channel_svd_1  os_channel_svd_2  os_channel_svd_3  \\\n0          0.632548          -0.05079         -0.045754          0.086897   \n1          0.632548          -0.05079         -0.045754          0.086897   \n2          0.632548          -0.05079         -0.045754          0.086897   \n3          0.632548          -0.05079         -0.045754          0.086897   \n4          0.632548          -0.05079         -0.045754          0.086897   \n\n   os_channel_svd_4  \n0           -0.3227  \n1           -0.3227  \n2           -0.3227  \n3           -0.3227  \n4           -0.3227  \n\n[5 rows x 94 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ip</th>\n      <th>app</th>\n      <th>device</th>\n      <th>os</th>\n      <th>channel</th>\n      <th>click_time</th>\n      <th>attributed_time</th>\n      <th>is_attributed</th>\n      <th>day</th>\n      <th>hour</th>\n      <th>...</th>\n      <th>device_channel_svd_0</th>\n      <th>device_channel_svd_1</th>\n      <th>device_channel_svd_2</th>\n      <th>device_channel_svd_3</th>\n      <th>device_channel_svd_4</th>\n      <th>os_channel_svd_0</th>\n      <th>os_channel_svd_1</th>\n      <th>os_channel_svd_2</th>\n      <th>os_channel_svd_3</th>\n      <th>os_channel_svd_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>27226</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>120</td>\n      <td>2017-11-06 15:13:23</td>\n      <td>None</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>110007</td>\n      <td>35</td>\n      <td>1</td>\n      <td>13</td>\n      <td>10</td>\n      <td>2017-11-06 15:41:07</td>\n      <td>2017-11-07 08:17:19</td>\n      <td>1</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1047</td>\n      <td>6</td>\n      <td>1</td>\n      <td>13</td>\n      <td>157</td>\n      <td>2017-11-06 15:42:32</td>\n      <td>None</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>76270</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>120</td>\n      <td>2017-11-06 15:56:17</td>\n      <td>None</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57862</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>120</td>\n      <td>2017-11-06 15:57:01</td>\n      <td>None</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 94 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "clicks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Feature Selection\n",
    "\n",
    "Using SelectKBest features. The F-value measures the linear dependency between the feature variable and the target. This means the score might underestimate the relation between a feature and the target if the relationship is nonlinear. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "feature_cols = clicks.columns.drop(['click_time', 'attributed_time', 'is_attributed'])\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the selector, keeping 40 features\n",
    "selector = SelectKBest(f_classif, k=40)\n",
    "\n",
    "# Use the selector to retrieve the best features\n",
    "X_new = selector.fit_transform(train[feature_cols], train['is_attributed']) \n",
    "\n",
    "# Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=feature_cols)\n",
    "\n",
    "# Find the columns that were dropped\n",
    "dropped_columns = selected_features.columns[selected_features.var() == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training model!\nValidation AUC score: 0.9625481759576047\n"
    }
   ],
   "source": [
    "_ = train_model(train.drop(dropped_columns, axis=1), \n",
    "                valid.drop(dropped_columns, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 regulatrization\n",
    "\n",
    "Univariate methods consider only one feature at a time when making a selection decision. Instead, we can make our selection using all of the features by including them in a linear model with L1 regularization. This type of regularization (sometimes called Lasso) penalizes the absolute magnitude of the coefficients, as compared to L2 (Ridge) regression which penalizes the square of the coefficients.\n",
    "\n",
    "Now try a more powerful approach using L1 regularization. Implement a function select_features_l1 that returns a list of features to keep.\n",
    "\n",
    "Use a LogisticRegression classifier model with an L1 penalty to select the features. For the model, set:\n",
    "\n",
    "the random state to 7,\n",
    "the regularization parameter to 0.1,\n",
    "and the solver to 'liblinear'.\n",
    "Fit the model then use SelectFromModel to return a model with the selected features.\n",
    "\n",
    "The checking code will run your function on a sample from the dataset to provide more immediate feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def select_features_l1(X, y):\n",
    "    \"\"\"Return selected features using logistic regression with an L1 penalty.\"\"\"\n",
    "    \n",
    "    logistic = LogisticRegression(C=0.1, penalty=\"l2\", random_state=7).fit(X, y)\n",
    "    model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "    X_new = model.transform(X)\n",
    "\n",
    "    # Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "    selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                    index=X.index,\n",
    "                                    columns=X.columns)\n",
    "\n",
    "    # Dropped columns have values of all 0s, keep other columns \n",
    "    cols_to_keep = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "    return cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training model!\nValidation AUC score: 0.9642664604988731\n"
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "X, y = train[feature_cols][:n_samples], train['is_attributed'][:n_samples]\n",
    "selected = select_features_l1(X, y)\n",
    "\n",
    "dropped_columns = feature_cols.drop(selected)\n",
    "_ = train_model(train.drop(dropped_columns, axis=1), \n",
    "                valid.drop(dropped_columns, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}